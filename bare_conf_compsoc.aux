\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Supervised learning and generalization}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Backpropagation in feedforward multi-layer networks}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Experiments}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Varying the effect of various parameters and measuring performance in terms of R value and training time.\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:res1}{{1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Conclusions}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Function approximation using feedforward multi-layered networks}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Dataset}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Investigating the effect of adding random noise to training data with standard deviation values from 0.1 to 0.4.\relax }}{3}}
\newlabel{fig:noise1}{{2}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Mean performance over 10 runs for different optimization algorithms.   \relax }}{3}}
\newlabel{fig:res1t}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Visualizing the personal regression surface\relax }}{4}}
\newlabel{fig:sur}{{3}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces MSE for the MLP with 100 neurons measured over 10 runs in the training and validation sets for the personal regression task\relax }}{4}}
\newlabel{tbl:reg1}{{2}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance of regression measured in the test set for the MP with 100 neurons \relax }}{4}}
\newlabel{fig:reg}{{3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Predicted against actual values for the personal regression task\relax }}{4}}
\newlabel{fig:regsum}{{4}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Validation performance for the personal regression task\relax }}{5}}
\newlabel{fig:regsum}{{5}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Training \& Performance Assessment}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Recurrent neural networks}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Hopfield Network}{5}}
\citation{santafe}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Time evolution in a 2D Hopfield network\relax }}{6}}
\newlabel{fig:hop}{{6}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Exampes of spurious attractor states in the Hopfield network\relax }}{6}}
\newlabel{fig:hop2}{{7}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Time series prediction using Recurrent Neural networks}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Time series prediction}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Number of correct (a), incorrect (b) and failed (c) digit retrievals for the hopefield network\relax }}{7}}
\newlabel{fig:hop2}{{8}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Experimental Setup}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Results and conclusions}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Time series prediction with the MLP (a) and the LSTM (b) network \relax }}{8}}
\newlabel{fig:santafe1}{{9}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Normalized Mean Squared error (NMSE) against the selected time delay p for the MLP and LSTM with 50,100 and 200 hidden units \relax }}{8}}
\newlabel{fig:rectune}{{10}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Deep feature learning}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Principal Component Analysis on Handwritten Digits}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Stacked Autoencoders}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Handwritten Digit Classification using Stacked Autoencoders}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Stacked Autoencoders}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Experimental Setup \& Results}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Convolutional Neural Networks}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Convolutional Neural networks}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Top: mean image of the database and visualizations of the first 4 principal components (left to right). Bottom: Reconstructing the digit image by using 4, 20, 80 and 256 principal components (left to right)\relax }}{10}}
\newlabel{fig:pca1}{{11}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Top: Reconstruction error, Bottom: Cumulative proportional variance explained\relax }}{10}}
\newlabel{fig:pca3}{{12}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The deployed stacked autoencoder architectures. Classification accuracy is measured by averaging the results over 10 runs.\relax }}{10}}
\newlabel{tableaut}{{4}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Handwritten Digit Classification using Convoluational Neural Networks}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Results for different activation functions\relax }}{11}}
\newlabel{acti}{{5}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Generative models}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Restriced Boltzmann Machines}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Visualization of activation maps for the 20, 40 and 60 filters of yhe three convolutional layers of architecture E\relax }}{12}}
\newlabel{fig:filters}{{13}{12}}
\newlabel{tab:table1_a}{{\caption@xref {tab:table1_a}{ on input line 766}}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Varying the number of filters with 1 convolutional layer\relax }}{12}}
\newlabel{cnnt1}{{6}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Varying the number of convolutional layers\relax }}{12}}
\newlabel{cnnt3}{{7}{12}}
\citation{mnist}
\citation{cifar}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Examples of underfitting when using a very small number of hidden units\relax }}{13}}
\newlabel{fig:rbm}{{14}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Examples of overfitting when using a very large number of hidden units\relax }}{13}}
\newlabel{fig:rbm2}{{15}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Deep Boltzmann Machines}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Generative Adversarial Networks}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Deep convolutional generative adversarial network (DCGAN)}{13}}
\citation{large}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Visualizing the filters learned in the first layer (a) and in the second layer (b) of a Deep Restricted Boltzman Machine\relax }}{14}}
\newlabel{fig:dpm}{{16}{14}}
\bibstyle{IEEEtran}
\bibdata{fdr_bib}
\bibcite{santafe}{1}
\bibcite{mnist}{2}
\bibcite{large}{3}
\bibcite{cifar}{4}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Training a GAN in cifar-10\relax }}{15}}
\newlabel{accs}{{17}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Optimal Transport}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Comparison of a fully connected minimax GAN and Wasserstein GAN}{15}}
\@writefile{toc}{\contentsline {section}{References}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Synthetically generated images of horses\relax }}{16}}
\newlabel{fig:gan}{{18}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Color transfer with Wasserstein and Sinkhorn algorithms\relax }}{16}}
\newlabel{fig:op}{{19}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Comparing the performance of WGAN and minmax GAN\relax }}{16}}
\newlabel{fig:last}{{20}{16}}
\@writefile{toc}{\contentsline {section}{Appendix}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Predicted values against actual values for the MLP with 3 hidden layers (first row). Mean squared error against number of epochs (second row). The number of hidden neurons in each layer are varied from 25 (a) to 200 (b) and 200 (c).\relax }}{18}}
\newlabel{fig:reg2}{{21}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Training, validation and test surfaces consisting of 1000 data points each (first row). Training and validation performance for Adam, Adadelta, Adagrad, Nadam and Adamax (second row). \relax }}{18}}
\newlabel{fig:surface}{{22}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Performance of regression measured by reporting the mean values for the Mean absolute error (MAE), Mean squared error (MSE), and $R^2$ over 10 runs. The deployed architecture consists of 3 hidden layers that have an equal amount of neurons in each layer.  \relax }}{18}}
\newlabel{fig:reg}{{8}{18}}
